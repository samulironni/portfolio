{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import gradio as gr\n",
    "from torchvision import models\n",
    "from torchvision.transforms import Compose, Resize, ToTensor\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preparing the Dataset and Data Loaders\n",
    "\n",
    "In this block, we define the necessary transformations for our dataset, load the data, and create data loaders for training and validation.\n",
    "\n",
    "### Transformations\n",
    "- **`Compose()`**: We use `Compose` to apply a series of transformations to the images.\n",
    "    - **`Resize((28, 28))`**: This resizes the images to a fixed size of 28x28 pixels, which is suitable for the model input.\n",
    "    - **`ToTensor()`**: This converts the images into tensors, which are required by PyTorch for model training.\n",
    "\n",
    "### Loading the Dataset\n",
    "- **`ImageFolder()`**: This utility is used to load images stored in folder structures. Each folder corresponds to a class label, and the images are automatically labeled based on the folder names.\n",
    "    - **`train_data`**: The training dataset is loaded from the `\"rps\"` directory, which contains images for Rock Paper Scissors.\n",
    "    - **`val_data`**: The validation dataset is loaded from the `\"rps-test-set\"` directory.\n",
    "    - **`train_data.classes`**: This prints out the class labels (Rock, Paper, Scissors) from the dataset.\n",
    "\n",
    "### Data Loaders\n",
    "- **`DataLoader()`**: This utility loads data in batches and shuffles it for training or validation.\n",
    "    - **`train_loader`**: Loads the training dataset with a batch size of 16 and shuffles the data.\n",
    "    - **`val_loader`**: Loads the validation dataset with a batch size of 16 but without shuffling.\n",
    "\n",
    "The final lines of the block print the number of samples in both the training and validation sets. These data loaders will be used to feed data into the model during training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: ['birch', 'juniper', 'maple', 'pine', 'spruce']\n",
      "Number of training samples: 106\n",
      "Number of validation samples: 30\n"
     ]
    }
   ],
   "source": [
    "# Transformations (from your lab)\n",
    "transform = Compose([Resize((128, 128)), ToTensor()])\n",
    "\n",
    "# Load the dataset\n",
    "train_data = ImageFolder(root='tree_train_set', transform=transform)\n",
    "val_data = ImageFolder(root='tree_test_set', transform=transform)\n",
    "print('Classes:', train_data.classes)\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_data, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=16, shuffle=False)\n",
    "print(f\"Number of training samples: {len(train_data)}\")\n",
    "print(f\"Number of validation samples: {len(val_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory: tree_test_set\n",
      "Number of files: 0\n",
      "Files: []\n",
      "--------------------------------------------------\n",
      "Directory: tree_test_set\\birch\n",
      "Number of files: 6\n",
      "Files: ['testBirch01-000.jpg', 'testBirch01-001.jpg', 'testBirch01-002.jpg', 'testBirch01-003.jpg', 'testBirch01-004.JPEG', 'testBirch01-005.JPEG']\n",
      "--------------------------------------------------\n",
      "Directory: tree_test_set\\juniper\n",
      "Number of files: 4\n",
      "Files: ['testJuniper02-000.jpg', 'testJuniper02-001.jpg', 'testJuniper02-002.jpg', 'testJuniper02-003.jpg']\n",
      "--------------------------------------------------\n",
      "Directory: tree_test_set\\maple\n",
      "Number of files: 9\n",
      "Files: ['testMaple03-000.jpg', 'testMaple03-001.jpg', 'testMaple03-002.jpg', 'testMaple03-003.jpg', 'testMaple03-004.jpg', 'testMaple03-005.jpg', 'testMaple03-006.jpg', 'testMaple03-007.jpg', 'testMaple03-008.jpg']\n",
      "--------------------------------------------------\n",
      "Directory: tree_test_set\\pine\n",
      "Number of files: 5\n",
      "Files: ['testPine04-000.JPEG', 'testPine04-001.JPEG', 'testPine04-002.jpg', 'testPine04-003.jpg', 'testPine04-004.jpg']\n",
      "--------------------------------------------------\n",
      "Directory: tree_test_set\\spruce\n",
      "Number of files: 6\n",
      "Files: ['testSpruce05-000.jpg', 'testSpruce05-001.jpg', 'testSpruce05-002.JPEG', 'testSpruce05-003.JPEG', 'testSpruce05-004.JPEG', 'testSpruce05-005.JPEG']\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "val_dir = 'tree_test_set'\n",
    "for root, dirs, files in os.walk(val_dir):\n",
    "    print(f\"Directory: {root}\")\n",
    "    print(f\"Number of files: {len(files)}\")\n",
    "    print(f\"Files: {files}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Loading a Pre-trained ResNet Model\n",
    "\n",
    "In this block, we load the pre-trained **ResNet18** model, which is a popular deep learning architecture known for its residual connections that help avoid the vanishing gradient problem in deep networks.\n",
    "\n",
    "- **`models.resnet18()`**: This function loads the **ResNet18** architecture from the **torchvision** library.\n",
    "    - **`weights=models.ResNet18_Weights.DEFAULT`**: This specifies that we are using the pre-trained weights provided by **torchvision**. These weights are trained on a large-scale dataset like ImageNet, allowing us to benefit from a model that has already learned useful features.\n",
    "- **`.to(device)`**: This moves the model to the specified device (usually a GPU or CPU). If a GPU is available, the model will run on it for faster computations.\n",
    "\n",
    "By using this pre-trained model, we leverage the knowledge it has already learned and adapt it to our specific task (transfer learning).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a pretrained ResNet model\n",
    "model = models.resnet18(weights=models.ResNet18_Weights.DEFAULT).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Displaying the Model Summary\n",
    "\n",
    "In this block, we use the **torchsummary** library to display a detailed summary of the **ResNet18** model architecture. This helps us understand the structure of the model and the number of parameters involved.\n",
    "\n",
    "- **`summary(model, input_size=(3, 224, 224))`**: This function provides a layer-by-layer summary of the model architecture.\n",
    "    - **Input size**: For ResNet18, the expected input size is a 3-channel image (RGB) with dimensions 224x224 pixels.\n",
    "\n",
    "### Model Summary\n",
    "- **Layer (type)**: Lists each layer in the model, such as convolutional layers, batch normalization, ReLU activations, and fully connected layers.\n",
    "- **Output Shape**: Displays the shape of the output after each layer.\n",
    "- **Param #**: Shows the number of parameters in each layer, including trainable parameters like weights and biases.\n",
    "\n",
    "The model has approximately **11.7 million parameters**, all of which are trainable. This is a relatively small number compared to deeper models like ResNet50, making ResNet18 a good choice for transfer learning tasks with limited computational resources.\n",
    "\n",
    "The summary provides the estimated memory usage for the input size, forward/backward passes, and the parameters, giving us a clear idea of the model's complexity and memory requirements.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 112, 112]           9,408\n",
      "       BatchNorm2d-2         [-1, 64, 112, 112]             128\n",
      "              ReLU-3         [-1, 64, 112, 112]               0\n",
      "         MaxPool2d-4           [-1, 64, 56, 56]               0\n",
      "            Conv2d-5           [-1, 64, 56, 56]          36,864\n",
      "       BatchNorm2d-6           [-1, 64, 56, 56]             128\n",
      "              ReLU-7           [-1, 64, 56, 56]               0\n",
      "            Conv2d-8           [-1, 64, 56, 56]          36,864\n",
      "       BatchNorm2d-9           [-1, 64, 56, 56]             128\n",
      "             ReLU-10           [-1, 64, 56, 56]               0\n",
      "       BasicBlock-11           [-1, 64, 56, 56]               0\n",
      "           Conv2d-12           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-13           [-1, 64, 56, 56]             128\n",
      "             ReLU-14           [-1, 64, 56, 56]               0\n",
      "           Conv2d-15           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-16           [-1, 64, 56, 56]             128\n",
      "             ReLU-17           [-1, 64, 56, 56]               0\n",
      "       BasicBlock-18           [-1, 64, 56, 56]               0\n",
      "           Conv2d-19          [-1, 128, 28, 28]          73,728\n",
      "      BatchNorm2d-20          [-1, 128, 28, 28]             256\n",
      "             ReLU-21          [-1, 128, 28, 28]               0\n",
      "           Conv2d-22          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-23          [-1, 128, 28, 28]             256\n",
      "           Conv2d-24          [-1, 128, 28, 28]           8,192\n",
      "      BatchNorm2d-25          [-1, 128, 28, 28]             256\n",
      "             ReLU-26          [-1, 128, 28, 28]               0\n",
      "       BasicBlock-27          [-1, 128, 28, 28]               0\n",
      "           Conv2d-28          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-29          [-1, 128, 28, 28]             256\n",
      "             ReLU-30          [-1, 128, 28, 28]               0\n",
      "           Conv2d-31          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-32          [-1, 128, 28, 28]             256\n",
      "             ReLU-33          [-1, 128, 28, 28]               0\n",
      "       BasicBlock-34          [-1, 128, 28, 28]               0\n",
      "           Conv2d-35          [-1, 256, 14, 14]         294,912\n",
      "      BatchNorm2d-36          [-1, 256, 14, 14]             512\n",
      "             ReLU-37          [-1, 256, 14, 14]               0\n",
      "           Conv2d-38          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-39          [-1, 256, 14, 14]             512\n",
      "           Conv2d-40          [-1, 256, 14, 14]          32,768\n",
      "      BatchNorm2d-41          [-1, 256, 14, 14]             512\n",
      "             ReLU-42          [-1, 256, 14, 14]               0\n",
      "       BasicBlock-43          [-1, 256, 14, 14]               0\n",
      "           Conv2d-44          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-45          [-1, 256, 14, 14]             512\n",
      "             ReLU-46          [-1, 256, 14, 14]               0\n",
      "           Conv2d-47          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-48          [-1, 256, 14, 14]             512\n",
      "             ReLU-49          [-1, 256, 14, 14]               0\n",
      "       BasicBlock-50          [-1, 256, 14, 14]               0\n",
      "           Conv2d-51            [-1, 512, 7, 7]       1,179,648\n",
      "      BatchNorm2d-52            [-1, 512, 7, 7]           1,024\n",
      "             ReLU-53            [-1, 512, 7, 7]               0\n",
      "           Conv2d-54            [-1, 512, 7, 7]       2,359,296\n",
      "      BatchNorm2d-55            [-1, 512, 7, 7]           1,024\n",
      "           Conv2d-56            [-1, 512, 7, 7]         131,072\n",
      "      BatchNorm2d-57            [-1, 512, 7, 7]           1,024\n",
      "             ReLU-58            [-1, 512, 7, 7]               0\n",
      "       BasicBlock-59            [-1, 512, 7, 7]               0\n",
      "           Conv2d-60            [-1, 512, 7, 7]       2,359,296\n",
      "      BatchNorm2d-61            [-1, 512, 7, 7]           1,024\n",
      "             ReLU-62            [-1, 512, 7, 7]               0\n",
      "           Conv2d-63            [-1, 512, 7, 7]       2,359,296\n",
      "      BatchNorm2d-64            [-1, 512, 7, 7]           1,024\n",
      "             ReLU-65            [-1, 512, 7, 7]               0\n",
      "       BasicBlock-66            [-1, 512, 7, 7]               0\n",
      "AdaptiveAvgPool2d-67            [-1, 512, 1, 1]               0\n",
      "           Linear-68                 [-1, 1000]         513,000\n",
      "================================================================\n",
      "Total params: 11,689,512\n",
      "Trainable params: 11,689,512\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 62.79\n",
      "Params size (MB): 44.59\n",
      "Estimated Total Size (MB): 107.96\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "# Display model summary (for an input size of (3, 224, 224) for ResNet)\n",
    "summary(model, input_size=(3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Print model architecture\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify the final fully connected layer for transfer learning (assuming 5 classes for the Trees dataset)\n",
    "model.fc = nn.Linear(512, 5).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Defining the Training and Validation Loops\n",
    "\n",
    "In this section, we define two key functions: one for training the model and one for validating its performance. These loops are fundamental to the transfer learning process, where the pre-trained model is fine-tuned on our specific dataset.\n",
    "\n",
    "### Training Loop\n",
    "\n",
    "The **`train()`** function handles the training process for one epoch, performing the following steps:\n",
    "\n",
    "- **`model.train()`**: Sets the model to training mode, enabling features like dropout and batch normalization.\n",
    "- **Forward pass**: The input images are passed through the model, and the output predictions are generated.\n",
    "- **Loss calculation**: The loss between the model's predictions and the true labels is computed using the specified criterion (e.g., cross-entropy loss).\n",
    "- **Backward pass**: The gradients of the loss with respect to the model parameters are computed (via `loss.backward()`), and the optimizer updates the model weights.\n",
    "- **Accuracy calculation**: The number of correctly predicted labels is compared against the total number of labels to calculate the training accuracy.\n",
    "\n",
    "### Validation Loop\n",
    "\n",
    "The **`validate()`** function handles the evaluation of the model on the validation set:\n",
    "\n",
    "- **`model.eval()`**: Sets the model to evaluation mode, disabling certain features like dropout.\n",
    "- **No gradient calculation**: The validation loop is wrapped in `torch.no_grad()` to prevent gradient calculations and save memory during the evaluation.\n",
    "- **Forward pass**: Similar to the training loop, the input images are passed through the model, but no backpropagation or optimization is performed.\n",
    "- **Accuracy and loss calculation**: The loss and accuracy are computed for the validation set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "def train(model, loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for inputs, labels in loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "    epoch_loss = running_loss / len(loader)\n",
    "    epoch_accuracy = 100. * correct / total\n",
    "\n",
    "    print(f\"Train Loss: {epoch_loss}, Train Accuracy: {epoch_accuracy}%\")\n",
    "\n",
    "# Validation loop\n",
    "def validate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "    epoch_loss = running_loss / len(loader)\n",
    "    epoch_accuracy = 100. * correct / total\n",
    "\n",
    "    print(f\"Validation Loss: {epoch_loss}, Validation Accuracy: {epoch_accuracy}%\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Training the Model\n",
    "\n",
    "In this block, we train the model for a set number of epochs using the previously defined **`train()`** and **`validate()`** functions. The model will learn from the training data and be evaluated on the validation set at the end of each epoch.\n",
    "\n",
    "- **`num_epochs = 10`**: Specifies that the model will be trained for 10 epochs. This can be adjusted based on the dataset size, model complexity, and available resources.\n",
    "- **Training Loop**: \n",
    "    - For each epoch, the **`train()`** function is called to train the model on the training set.\n",
    "    - After each epoch, the **`validate()`** function evaluates the model on the validation set to track its performance.\n",
    "    - For each epoch, the loss and accuracy for both the training and validation sets are printed to monitor progress.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "Train Loss: 0.9396577605179378, Train Accuracy: 61.320754716981135%\n",
      "Validation Loss: 5.3167808055877686, Validation Accuracy: 40.0%\n",
      "Epoch 2/10\n",
      "Train Loss: 0.3089677739356245, Train Accuracy: 91.50943396226415%\n",
      "Validation Loss: 1.7071059048175812, Validation Accuracy: 56.666666666666664%\n",
      "Epoch 3/10\n",
      "Train Loss: 0.25720760811652454, Train Accuracy: 90.56603773584905%\n",
      "Validation Loss: 2.906321167945862, Validation Accuracy: 60.0%\n",
      "Epoch 4/10\n",
      "Train Loss: 0.296782414828028, Train Accuracy: 91.50943396226415%\n",
      "Validation Loss: 2.844532608985901, Validation Accuracy: 66.66666666666667%\n",
      "Epoch 5/10\n",
      "Train Loss: 0.48130270200116293, Train Accuracy: 87.73584905660377%\n",
      "Validation Loss: 3.256892681121826, Validation Accuracy: 56.666666666666664%\n",
      "Epoch 6/10\n",
      "Train Loss: 0.11259050933378083, Train Accuracy: 97.16981132075472%\n",
      "Validation Loss: 4.926035523414612, Validation Accuracy: 43.333333333333336%\n",
      "Epoch 7/10\n",
      "Train Loss: 0.11549123349998679, Train Accuracy: 96.22641509433963%\n",
      "Validation Loss: 2.9049673676490784, Validation Accuracy: 73.33333333333333%\n",
      "Epoch 8/10\n",
      "Train Loss: 0.051209098526409695, Train Accuracy: 99.05660377358491%\n",
      "Validation Loss: 1.7091024518013, Validation Accuracy: 76.66666666666667%\n",
      "Epoch 9/10\n",
      "Train Loss: 0.13918105363180594, Train Accuracy: 95.28301886792453%\n",
      "Validation Loss: 1.2659282982349396, Validation Accuracy: 70.0%\n",
      "Epoch 10/10\n",
      "Train Loss: 0.16894223541021347, Train Accuracy: 97.16981132075472%\n",
      "Validation Loss: 1.0487462878227234, Validation Accuracy: 83.33333333333333%\n"
     ]
    }
   ],
   "source": [
    "# Train the model for 10 epochs (you can increase the epochs as needed)\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}')\n",
    "    train(model, train_loader, criterion, optimizer, device)\n",
    "    validate(model, val_loader, criterion, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to save the model to disk \n",
    "#torch.save(model.state_dict(), 'resnet18_transfer_learning.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: maple\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "# Function to perform inference on a single image\n",
    "def infer(model, image_path, transform, device):\n",
    "    # Load the image and convert it to RGB (3 channels)\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    image = transform(image).unsqueeze(0).to(device)  # Add batch dimension\n",
    "\n",
    "    # Put the model in evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(image)\n",
    "        _, predicted = output.max(1)\n",
    "    \n",
    "    return predicted.item()\n",
    "\n",
    "# Class names are based on the order printed from train_data.classes\n",
    "class_names = ['birch', 'juniper', 'maple', 'pine', 'spruce']\n",
    "\n",
    "# Path to the test image\n",
    "image_path = 'tree_test_set/pine/testpine04-002.jpg'  # Adjust the path as needed\n",
    "\n",
    "# Run inference\n",
    "predicted_class = infer(model, image_path, transform, device)\n",
    "print(f'Predicted class: {class_names[predicted_class]}')\n",
    "\n",
    "# Path to the test image\n",
    "# image_path = 'rps-test-set/rock/testrock01-00.png'  # Ensure the path is correct\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install scikit-learn matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tehdään tuloksista confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of predictions: 30\n",
      "Number of true labels: 30\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Function to get predictions for the entire validation set\n",
    "def get_all_predictions(model, loader, device):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = outputs.max(1)\n",
    "            all_preds.append(predicted.cpu().numpy())\n",
    "            all_labels.append(labels.cpu().numpy())\n",
    "\n",
    "    # Convert list of arrays to single numpy arrays\n",
    "    all_preds = np.concatenate(all_preds)\n",
    "    all_labels = np.concatenate(all_labels)\n",
    "    return all_preds, all_labels\n",
    "\n",
    "# Get predictions and true labels from validation data\n",
    "preds, labels = get_all_predictions(model, val_loader, device)\n",
    "# Print the shape of preds and labels\n",
    "print(f\"Number of predictions: {len(preds)}\")\n",
    "print(f\"Number of true labels: {len(labels)}\")\n",
    "# Compute the confusion matrix\n",
    "conf_matrix = confusion_matrix(labels, preds)\n",
    "\n",
    "# Display the confusion matrix\n",
    "class_names = ['birch', 'juniper', 'maple', 'pine', 'spruce']  # Based on your dataset\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix, display_labels=class_names)\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "def predict(image):\n",
    "    try:\n",
    "        # Convert the image to a tensor\n",
    "        if not isinstance(image, torch.Tensor):\n",
    "            # First, convert the PIL image to a tensor\n",
    "            transform = ToTensor()\n",
    "            image = transform(image).unsqueeze(0).to(device)  # Add batch dimension and move to device\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Get model output\n",
    "            output = model(image)\n",
    "\n",
    "            # Apply softmax to get probabilities for each class\n",
    "            probabilities = torch.nn.functional.softmax(output, dim=1)\n",
    "\n",
    "            # Convert probabilities to percentages\n",
    "            scores = probabilities.squeeze().cpu().numpy() * 100\n",
    "\n",
    "            # Map scores to class labels\n",
    "            class_names = train_data.classes  # Automatically gets class names from dataset\n",
    "            class_scores = {class_names[i]: scores[i] for i in range(len(class_names))}\n",
    "\n",
    "        # Sort the classes by highest score\n",
    "        sorted_class_scores = sorted(class_scores.items(), key=lambda item: item[1], reverse=True)\n",
    "\n",
    "        # Get the class with the highest score\n",
    "        most_likely_class = sorted_class_scores[0]\n",
    "\n",
    "        # Format the result to display the class and its score\n",
    "        result = f\"{most_likely_class}\"\n",
    "\n",
    "        return result\n",
    "\n",
    "    except Exception as e:\n",
    "        # Return the error message if something goes wrong\n",
    "        return f\"Error: {str(e)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradio Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7863\n",
      "Running on public URL: https://9b3407bf2f3a3a8cde.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://9b3407bf2f3a3a8cde.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the Gradio interface\n",
    "iface = gr.Interface(\n",
    "    fn=predict,\n",
    "    inputs=gr.Image(type=\"pil\"),\n",
    "    outputs=\"text\",\n",
    "    title=\"Tree Species Classifier\",\n",
    "    description=\"Upload an image of a tree, and the model will predict the species with confidence scores for each class.\"\n",
    ")\n",
    "\n",
    "# Launch the Gradio app\n",
    "iface.launch(share=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Inference on a Single Image and Logging Results to W&B\n",
    "\n",
    "In this block, we define a function to perform inference on a single image using the pre-trained model and log the results to **Weights & Biases (W&B)**. This is useful for evaluating the model's performance on individual images and visualizing the results.\n",
    "\n",
    "### Function: `infer_and_log()`\n",
    "\n",
    "- **Loading and Preprocessing**: \n",
    "    - **`Image.open(image_path).convert('RGB')`**: Opens the image from the given path and converts it to an RGB format (3-channel).\n",
    "    - **`transform(image)`**: Applies the transformation (e.g., resizing, tensor conversion) to the image, similar to how we transformed the training and validation images.\n",
    "    - **`.unsqueeze(0)`**: Adds a batch dimension to the image tensor, as the model expects input in batches.\n",
    "    - The image tensor is then moved to the specified device (GPU or CPU) using **`.to(device)`**.\n",
    "\n",
    "- **Inference**:\n",
    "    - **`model.eval()`**: The model is set to evaluation mode to disable any training-specific operations.\n",
    "    - **`torch.no_grad()`**: Disables gradient calculations to save memory and speed up inference.\n",
    "    - The model processes the image, and the predicted class is obtained by finding the index of the highest output score using **`.max(1)`**.\n",
    "    - The predicted class is then mapped to its corresponding label using the **`class_names`** list.\n",
    "\n",
    "- **Logging to Weights & Biases**:\n",
    "    - **`wandb.log()`**: Logs the original input image along with the predicted class to **W&B** for easy tracking and visualization.\n",
    "    - **`wandb.Image()`**: This function is used to log the input image along with a caption showing the predicted class.\n",
    "\n",
    "### Example Usage\n",
    "\n",
    "- **`class_names`**: The list of class names for the Rock, Paper, Scissors dataset (`['paper', 'rock', 'scissors']`).\n",
    "- **Image Path**: The path to a test image (e.g., `'rps-test-set/rock/testrock01-00.png'`).\n",
    "- **Inference and Logging**: The `infer_and_log()` function is called, which performs inference on the test image and logs the results to **W&B**.\n",
    "\n",
    "The predicted class is then printed to the console to verify the result.\n",
    "\n",
    "By using this function, you can easily evaluate the model's predictions on individual images and track those predictions through **Weights & Biases** for better analysis and visualization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "import wandb\n",
    "\n",
    "# Function to perform inference on a single image and log results to W&B\n",
    "def infer_and_log(model, image_path, transform, device, class_names):\n",
    "    # Load and preprocess the image\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    image_tensor = transform(image).unsqueeze(0).to(device)  # Add batch dimension\n",
    "\n",
    "    # Put the model in evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Run inference\n",
    "    with torch.no_grad():\n",
    "        outputs = model(image_tensor)\n",
    "        _, predicted = outputs.max(1)\n",
    "        predicted_class = class_names[predicted.item()]\n",
    "    \n",
    "    # Log the input image and the prediction to W&B\n",
    "    wandb.log({\n",
    "        \"Image\": wandb.Image(image, caption=f\"Predicted: {predicted_class}\"),\n",
    "        \"Prediction\": predicted_class\n",
    "    })\n",
    "\n",
    "    return predicted_class\n",
    "\n",
    "# Example usage\n",
    "# Assuming class_names = ['paper', 'rock', 'scissors']\n",
    "class_names = ['paper', 'rock', 'scissors']\n",
    "\n",
    "# Path to the test image\n",
    "image_path = 'rps-test-set/rock/testrock01-00.png'\n",
    "\n",
    "# Run inference and log results to W&B\n",
    "predicted_class = infer_and_log(model, image_path, transform, device, class_names)\n",
    "print(f'Predicted class: {predicted_class}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "devAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
